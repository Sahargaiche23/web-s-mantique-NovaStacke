# üö® **PROBL√àME OLLAMA - GUIDE DE R√âSOLUTION**

## ‚ùå **Probl√®me Identifi√©**
**Erreur 429 (quota insuffisant) d'OpenAI au lieu d'utiliser Ollama**

---

## üõ†Ô∏è **Solution √âtape par √âtape**

### **√âtape 1 : V√©rifier la Configuration**
```bash
cd /home/sahar/CascadeProjects/eco_travel_semantic/backend

# V√©rifier le fichier .env
cat .env

# R√©sultat attendu :
# USE_OLLAMA=true
# OLLAMA_BASE_URL=http://127.0.0.1:11434
# OLLAMA_MODEL=qwen2.5:3b-instruct
```

### **√âtape 2 : D√©marrer Ollama**
```bash
# Terminal 1 - D√©marrer Ollama
ollama serve

# V√©rifier qu'Ollama r√©pond
curl -s http://localhost:11434/api/version
# R√©sultat attendu : {"version":"0.12.3"}
```

### **√âtape 3 : V√©rifier le Mod√®le**
```bash
# V√©rifier les mod√®les install√©s
ollama list

# R√©sultat attendu : qwen2.5:3b-instruct doit √™tre pr√©sent
# Sinon l'installer :
ollama pull qwen2.5:3b-instruct
```

### **√âtape 4 : Test du Chatbot**
```bash
cd /home/sahar/CascadeProjects/eco_travel_semantic/backend

# Charger les variables d'environnement
export $(cat .env | grep -v '^#' | xargs)

# Test du chatbot avec Ollama
./venv/bin/python -c "
import os
print('Variables d environnement:')
print(f'USE_OLLAMA: {os.getenv(\"USE_OLLAMA\")}')
print(f'OLLAMA_BASE_URL: {os.getenv(\"OLLAMA_BASE_URL\")}')
print(f'OLLAMA_MODEL: {os.getenv(\"OLLAMA_MODEL\")}')

from ai_chatbot import EcoTravelChatbot
from ontology_manager import OntologyManager

onto = OntologyManager('ontology/ecotourisme.owl')
onto.load_ontology()
chatbot = EcoTravelChatbot(onto)

result = chatbot.process_message('Test', use_llm=True)
print('‚úÖ R√©ponse:', result['response'][:100])
"
```

### **√âtape 5 : D√©marrer l'Application**
```bash
# Utiliser le script corrig√©
./start_with_ollama.sh
```

---

## üéØ **V√©rifications**

### **‚úÖ V√©rification 1 : Variables d'Environnement**
```bash
export $(cat .env | grep -v '^#' | xargs)
echo $USE_OLLAMA
echo $OLLAMA_BASE_URL
echo $OLLAMA_MODEL
```

### **‚úÖ V√©rification 2 : Ollama**
```bash
curl -s http://localhost:11434/api/version
ollama list
```

### **‚úÖ V√©rification 3 : Application**
```bash
curl -s http://localhost:5000/api/health
```

---

## üö® **Si Probl√®me Persiste**

### **Option 1 : Red√©marrer Compl√®tement**
```bash
# Arr√™ter tout
pkill -f ollama
pkill -f "python app.py"

# Red√©marrer
ollama serve &
sleep 3
cd /home/sahar/CascadeProjects/eco_travel_semantic/backend
export $(cat .env | grep -v '^#' | xargs)
./venv/bin/python app.py
```

### **Option 2 : Debug du Chatbot**
```bash
cd /home/sahar/CascadeProjects/eco_travel_semantic/backend

# Test direct du bridge LLM
./venv/bin/python -c "
from llm_bridge import LLMAssistant
import os

# Forcer Ollama
os.environ['USE_OLLAMA'] = 'true'
os.environ['OLLAMA_BASE_URL'] = 'http://127.0.0.1:11434'
os.environ['OLLAMA_MODEL'] = 'qwen2.5:3b-instruct'

llm = LLMAssistant()
response = llm.complete('Bonjour, comment allez-vous?')
print('‚úÖ Test LLM:', response[:100])
"
```

---

## üéâ **Test Final**

**Une fois tout d√©marr√© :**
1. **Ouvrir** : http://localhost:5000
2. **Aller √†** : "Chatbot"
3. **Activer** : "ü§ñ Utiliser IA"
4. **Demander** : "Quelles sont les meilleures destinations √©cologiques?"
5. **‚úÖ R√©sultat** : R√©ponse avec Ollama (pas d'erreur 429)

---

## üîß **R√©solution D√©finitive**

**Le probl√®me √©tait que :**
- ‚ùå **Les variables d'environnement n'√©taient pas charg√©es**
- ‚ùå **Le serveur utilisait encore OpenAI**
- ‚úÖ **Script corrig√© pour charger correctement les variables**
- ‚úÖ **Ollama configur√© et fonctionnel**

**üåü Suivez ce guide √©tape par √©tape et tout fonctionnera parfaitement !**
